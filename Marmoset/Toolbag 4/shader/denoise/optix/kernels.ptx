//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30411180
// Cuda compilation tools, release 11.5, V11.5.50
// Based on NVVM 7.0.1
//

.version 7.5
.target sm_50
.address_size 64

	// .globl	loadKernel

.visible .entry loadKernel(
	.param .u32 loadKernel_param_0,
	.param .u32 loadKernel_param_1,
	.param .u64 loadKernel_param_2,
	.param .u64 loadKernel_param_3,
	.param .u64 loadKernel_param_4,
	.param .u64 loadKernel_param_5,
	.param .u64 loadKernel_param_6,
	.param .u64 loadKernel_param_7,
	.param .u64 loadKernel_param_8
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r3, [loadKernel_param_0];
	ld.param.u32 	%r4, [loadKernel_param_1];
	ld.param.u64 	%rd2, [loadKernel_param_2];
	ld.param.u64 	%rd3, [loadKernel_param_3];
	ld.param.u64 	%rd4, [loadKernel_param_4];
	ld.param.u64 	%rd5, [loadKernel_param_5];
	ld.param.u64 	%rd6, [loadKernel_param_6];
	ld.param.u64 	%rd7, [loadKernel_param_7];
	ld.param.u64 	%rd8, [loadKernel_param_8];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_5;

	cvta.to.global.u64 	%rd9, %rd3;
	shl.b32 	%r11, %r1, 3;
	suld.b.2d.v4.b16.trap {%rs1, %rs2, %rs3, %rs4}, [%rd2, {%r11, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	mov.f32 	%f5, 0f461C4000;
	min.f32 	%f6, %f1, %f5;
	mov.f32 	%f7, 0f00000000;
	min.f32 	%f8, %f2, %f5;
	min.f32 	%f9, %f3, %f5;
	mad.lo.s32 	%r12, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r12;
	mul.wide.u32 	%rd10, %r12, 16;
	add.s64 	%rd11, %rd9, %rd10;
	max.f32 	%f10, %f7, %f9;
	max.f32 	%f11, %f7, %f8;
	max.f32 	%f12, %f7, %f6;
	// begin inline asm
	{  cvt.f32.f16 %f4, %rs4;}

	// end inline asm
	st.global.v4.f32 	[%rd11], {%f12, %f11, %f10, %f4};
	setp.eq.s64 	%p4, %rd4, 0;
	@%p4 bra 	$L__BB0_3;

	shl.b32 	%r13, %r1, 2;
	mov.u32 	%r14, 0;
	suld.b.a2d.v4.b8.trap {%rs5, %rs6, %rs7, %rs8}, [%rd4, {%r14, %r13, %r2, %r2}];
	and.b16  	%rs9, %rs5, 255;
	and.b16  	%rs10, %rs6, 255;
	and.b16  	%rs11, %rs7, 255;
	and.b16  	%rs12, %rs8, 255;
	cvt.rn.f32.u16 	%f13, %rs11;
	cvt.rn.f32.u16 	%f14, %rs10;
	cvt.rn.f32.u16 	%f15, %rs9;
	cvt.rn.f32.u16 	%f16, %rs12;
	mov.u32 	%r15, 1;
	suld.b.a2d.v4.b8.trap {%rs13, %rs14, %rs15, %rs16}, [%rd4, {%r15, %r13, %r2, %r2}];
	and.b16  	%rs17, %rs14, 255;
	and.b16  	%rs18, %rs15, 255;
	and.b16  	%rs19, %rs16, 255;
	cvt.rn.f32.u16 	%f17, %rs18;
	mul.f32 	%f18, %f17, 0f3B808081;
	cvt.rn.f32.u16 	%f19, %rs17;
	mul.f32 	%f20, %f19, 0f3B808081;
	cvt.rn.f32.u16 	%f21, %rs19;
	mul.f32 	%f22, %f16, 0f3B808081;
	mul.f32 	%f23, %f15, 0f3B808081;
	mul.f32 	%f24, %f14, 0f3B808081;
	mul.f32 	%f25, %f13, 0f3B808081;
	cvta.to.global.u64 	%rd12, %rd5;
	shl.b64 	%rd13, %rd1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.v4.f32 	[%rd14], {%f25, %f24, %f23, %f22};
	mul.f32 	%f26, %f21, 0f3B808081;
	fma.rn.f32 	%f27, %f20, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f28, %f18, 0f40000000, 0fBF800000;
	cvta.to.global.u64 	%rd15, %rd6;
	add.s64 	%rd16, %rd15, %rd13;
	st.global.v4.f32 	[%rd16], {%f28, %f27, %f7, %f26};

$L__BB0_3:
	setp.eq.s64 	%p5, %rd7, 0;
	@%p5 bra 	$L__BB0_5;

	shl.b32 	%r16, %r1, 2;
	suld.b.2d.b32.trap {%r17}, [%rd7, {%r16, %r2}];
	cvta.to.global.u64 	%rd17, %rd8;
	shl.b64 	%rd18, %rd1, 2;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %r17;

$L__BB0_5:
	ret;

}
	// .globl	loadAccumulateKernel
.visible .entry loadAccumulateKernel(
	.param .u32 loadAccumulateKernel_param_0,
	.param .u32 loadAccumulateKernel_param_1,
	.param .u64 loadAccumulateKernel_param_2,
	.param .u64 loadAccumulateKernel_param_3,
	.param .u64 loadAccumulateKernel_param_4,
	.param .u64 loadAccumulateKernel_param_5,
	.param .u64 loadAccumulateKernel_param_6,
	.param .u64 loadAccumulateKernel_param_7,
	.param .u64 loadAccumulateKernel_param_8,
	.param .f32 loadAccumulateKernel_param_9
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<81>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<20>;


	ld.param.u32 	%r3, [loadAccumulateKernel_param_0];
	ld.param.u32 	%r4, [loadAccumulateKernel_param_1];
	ld.param.u64 	%rd2, [loadAccumulateKernel_param_2];
	ld.param.u64 	%rd3, [loadAccumulateKernel_param_3];
	ld.param.u64 	%rd4, [loadAccumulateKernel_param_4];
	ld.param.u64 	%rd5, [loadAccumulateKernel_param_5];
	ld.param.u64 	%rd6, [loadAccumulateKernel_param_6];
	ld.param.u64 	%rd7, [loadAccumulateKernel_param_7];
	ld.param.u64 	%rd8, [loadAccumulateKernel_param_8];
	ld.param.f32 	%f2, [loadAccumulateKernel_param_9];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_5;

	cvta.to.global.u64 	%rd9, %rd3;
	shl.b32 	%r11, %r1, 3;
	suld.b.2d.v4.b16.trap {%rs1, %rs2, %rs3, %rs4}, [%rd2, {%r11, %r2}];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f4, %rs2;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs3;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs4;}

	// end inline asm
	mad.lo.s32 	%r12, %r2, %r3, %r1;
	cvt.u64.u32 	%rd1, %r12;
	mul.wide.u32 	%rd10, %r12, 16;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.v4.f32 	{%f7, %f8, %f9, %f10}, [%rd11];
	mov.f32 	%f15, 0f461C4000;
	min.f32 	%f16, %f3, %f15;
	mov.f32 	%f17, 0f00000000;
	max.f32 	%f18, %f17, %f16;
	min.f32 	%f19, %f4, %f15;
	max.f32 	%f20, %f17, %f19;
	min.f32 	%f21, %f5, %f15;
	max.f32 	%f22, %f17, %f21;
	mov.f32 	%f23, 0f3F800000;
	sub.f32 	%f1, %f23, %f2;
	mul.f32 	%f24, %f18, %f2;
	mul.f32 	%f25, %f20, %f2;
	mul.f32 	%f26, %f22, %f2;
	mul.f32 	%f27, %f1, %f10;
	fma.rn.f32 	%f28, %f6, %f2, %f27;
	fma.rn.f32 	%f29, %f1, %f9, %f26;
	fma.rn.f32 	%f30, %f1, %f8, %f25;
	fma.rn.f32 	%f31, %f1, %f7, %f24;
	st.global.v4.f32 	[%rd11], {%f31, %f30, %f29, %f28};
	setp.eq.s64 	%p4, %rd4, 0;
	@%p4 bra 	$L__BB1_3;

	shl.b32 	%r13, %r1, 2;
	mov.u32 	%r14, 0;
	suld.b.a2d.v4.b8.trap {%rs5, %rs6, %rs7, %rs8}, [%rd4, {%r14, %r13, %r2, %r2}];
	and.b16  	%rs9, %rs5, 255;
	and.b16  	%rs10, %rs6, 255;
	and.b16  	%rs11, %rs7, 255;
	and.b16  	%rs12, %rs8, 255;
	cvt.rn.f32.u16 	%f32, %rs11;
	mul.f32 	%f33, %f32, 0f3B808081;
	cvt.rn.f32.u16 	%f34, %rs10;
	mul.f32 	%f35, %f34, 0f3B808081;
	cvt.rn.f32.u16 	%f36, %rs9;
	mul.f32 	%f37, %f36, 0f3B808081;
	cvt.rn.f32.u16 	%f38, %rs12;
	mul.f32 	%f39, %f38, 0f3B808081;
	mov.u32 	%r15, 1;
	suld.b.a2d.v4.b8.trap {%rs13, %rs14, %rs15, %rs16}, [%rd4, {%r15, %r13, %r2, %r2}];
	and.b16  	%rs17, %rs14, 255;
	and.b16  	%rs18, %rs15, 255;
	and.b16  	%rs19, %rs16, 255;
	cvt.rn.f32.u16 	%f40, %rs18;
	mul.f32 	%f41, %f40, 0f3B808081;
	cvt.rn.f32.u16 	%f42, %rs17;
	mul.f32 	%f43, %f42, 0f3B808081;
	cvt.rn.f32.u16 	%f44, %rs19;
	mul.f32 	%f45, %f44, 0f3B808081;
	cvta.to.global.u64 	%rd12, %rd5;
	shl.b64 	%rd13, %rd1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.v4.f32 	{%f46, %f47, %f48, %f49}, [%rd14];
	cvta.to.global.u64 	%rd15, %rd6;
	add.s64 	%rd16, %rd15, %rd13;
	ld.global.v4.f32 	{%f54, %f55, %f56, %f57}, [%rd16];
	fma.rn.f32 	%f62, %f41, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f63, %f43, 0f40000000, 0fBF800000;
	mul.f32 	%f64, %f33, %f2;
	mul.f32 	%f65, %f35, %f2;
	mul.f32 	%f66, %f37, %f2;
	fma.rn.f32 	%f67, %f1, %f48, %f66;
	fma.rn.f32 	%f68, %f1, %f47, %f65;
	fma.rn.f32 	%f69, %f1, %f46, %f64;
	max.f32 	%f70, %f49, %f39;
	st.global.v4.f32 	[%rd14], {%f69, %f68, %f67, %f70};
	mul.f32 	%f71, %f62, %f2;
	mul.f32 	%f72, %f63, %f2;
	mul.f32 	%f73, %f1, %f56;
	fma.rn.f32 	%f74, %f1, %f55, %f72;
	fma.rn.f32 	%f75, %f1, %f54, %f71;
	max.f32 	%f76, %f57, %f45;
	fma.rn.f32 	%f77, %f2, 0f00000000, %f73;
	st.global.v4.f32 	[%rd16], {%f75, %f74, %f77, %f76};

$L__BB1_3:
	setp.eq.s64 	%p5, %rd7, 0;
	@%p5 bra 	$L__BB1_5;

	shl.b32 	%r16, %r1, 2;
	suld.b.2d.b32.trap {%r17}, [%rd7, {%r16, %r2}];
	mov.b32 	%f78, %r17;
	cvta.to.global.u64 	%rd17, %rd8;
	shl.b64 	%rd18, %rd1, 2;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.f32 	%f79, [%rd19];
	max.f32 	%f80, %f79, %f78;
	st.global.f32 	[%rd19], %f80;

$L__BB1_5:
	ret;

}
	// .globl	storeKernel
.visible .entry storeKernel(
	.param .u32 storeKernel_param_0,
	.param .u32 storeKernel_param_1,
	.param .u64 storeKernel_param_2,
	.param .u64 storeKernel_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<5>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r3, [storeKernel_param_0];
	ld.param.u32 	%r4, [storeKernel_param_1];
	ld.param.u64 	%rd1, [storeKernel_param_2];
	ld.param.u64 	%rd2, [storeKernel_param_3];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB2_2;

	mul.lo.s32 	%r11, %r2, %r3;
	cvt.u64.u32 	%rd4, %r11;
	cvt.u64.u32 	%rd5, %r1;
	add.s64 	%rd6, %rd4, %rd5;
	shl.b64 	%rd7, %rd6, 4;
	add.s64 	%rd3, %rd2, %rd7;
	// begin inline asm
	ld.global.nc.v4.f32 {%f1,%f2,%f3,%f4}, [%rd3];
	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f4;}

	// end inline asm
	shl.b32 	%r12, %r1, 3;
	sust.b.2d.v4.b16.trap 	[%rd1, {%r12, %r2}], {%rs1, %rs2, %rs3, %rs4};

$L__BB2_2:
	ret;

}
	// .globl	storeDepthKernel
.visible .entry storeDepthKernel(
	.param .u32 storeDepthKernel_param_0,
	.param .u32 storeDepthKernel_param_1,
	.param .u64 storeDepthKernel_param_2,
	.param .u64 storeDepthKernel_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r3, [storeDepthKernel_param_0];
	ld.param.u32 	%r4, [storeDepthKernel_param_1];
	ld.param.u64 	%rd1, [storeDepthKernel_param_2];
	ld.param.u64 	%rd2, [storeDepthKernel_param_3];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.u32 	%p1, %r1, %r3;
	setp.ge.u32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB3_2;

	mul.lo.s32 	%r11, %r2, %r3;
	cvt.u64.u32 	%rd4, %r11;
	cvt.u64.u32 	%rd5, %r1;
	add.s64 	%rd6, %rd4, %rd5;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd3, %rd2, %rd7;
	// begin inline asm
	ld.global.nc.f32 %f1, [%rd3];
	// end inline asm
	mov.b32 	%r12, %f1;
	shl.b32 	%r13, %r1, 2;
	sust.b.2d.b32.trap 	[%rd1, {%r13, %r2}], {%r12};

$L__BB3_2:
	ret;

}

